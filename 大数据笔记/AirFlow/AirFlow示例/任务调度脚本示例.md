# 项目任务调度

## sqoop导数据任务

-   全量导数据任务

    -   sqoop_full_import_tables.py

        ```python
        # -*- coding:utf-8 -*-
        #__author__ = 'laowei'
        from datetime import timedelta
        from airflow import DAG
        from airflow.operators.bash import BashOperator
        from airflow.utils.dates import days_ago
        
        default_args = {
            'owner': 'airflow',
            'email': ['airflow@example.com'],
            'email_on_failure': False,
            'email_on_retry': False,
            'retries': 1,
            'retry_delay': timedelta(minutes=3)
        }
        
        dag = DAG(
            'sqoop_full_import_tables',
            default_args=default_args,
            schedule_interval='* * 1 1 *',
            start_date=days_ago(1),
            dagrun_timeout=timedelta(minutes=5),
            template_searchpath=['/root/airflow/dags/OneMake/sqoop_script'],
            tags=['itcast_sqooptask']
        )
        
        sqoop_full_import_task = BashOperator(
            task_id='full_import_tables_task',
            bash_command='full_import_tables.sh',
            dag=dag)
        
        sqoop_full_import_task
        ```

-   增量调度任务

    -   sqoop_incr_import_tables.py

        ```python
        # -*- coding:utf-8 -*-
        #__author__ = 'laowei'
        from datetime import timedelta
        from airflow import DAG
        from airflow.operators.bash import BashOperator
        from airflow.utils.dates import days_ago
        
        default_args = {
            'owner': 'airflow',
            'email': ['airflow@example.com'],
            'email_on_failure': False,
            'email_on_retry': False,
            'retries': 1,
            'retry_delay': timedelta(minutes=3)
        }
        
        dag = DAG(
            'sqoop_incr_import_tables',
            default_args=default_args,
            schedule_interval='* * 1 * *',
            start_date=days_ago(1),
            dagrun_timeout=timedelta(minutes=5),
            template_searchpath=['/root/airflow/dags/OneMake/sqoop_script'],
            tags=['itcast_sqooptask']
        )
        
        sqoop_incr_import_task = BashOperator(
            task_id='incr_import_tables_task',
            bash_command='incr_import_tables.sh',
            dag=dag)
        
        sqoop_incr_import_task
        ```

## 创建ods&dwd表与加载数据任务

-   使用开发工具对模块打包

-   使用java命令在服务器上手动执行

    -   打包并上传jar，使用java命令执行

    ```java
    // 配置了main class，可以直接java -jar执行jar包
    java -jar AutoCreateHiveTable-1.0-SNAPSHOT-jar-with-dependencies.jar
    // 有多个主类的情况下，可以在jar包后面加上全路径主类
    java -jar jar包 主类
    ```

## sparksql加载数据任务

-   以dwb中呼叫中心事实表为例

    -   fact_call_service.hql

        ```sql
        insert overwrite table one_make_dwb.fact_call_service2 partition (dt = '20210101')
        select
            call.id
            , call.code
            , date_format(timestamp(call.call_time), 'yyyymmdd') as call_date
            , hour(timestamp(call.call_time))
            , call.call_type
            , call_dict.dictname
            , call.process_way
            , process_dict.dictname
            , call.call_oilstation_id
            , call.accept_userid
            , 1
            , case when call.process_way = 5  then 1 else 0 end
            , case when workorder.status = -1 then 1 else 0 end
            , case when workorder.status = -2 then 1 else 0 end
            , floor(to_unix_timestamp(timestamp(call.process_time),'yyyy-mm-dd hhmmss') - to_unix_timestamp(timestamp(call.call_time), 'yyyy-mm-dd hhmmss')  1000.0)
            , case when call.call_type = 5 then 1 else 0 end
            , case when call.call_type in (1, 2, 3, 4) then 1 else 0 end
            , case when call.call_type = 7 then 1 else 0 end
            , case when call.call_type = 8 then 1 else 0 end
            , case when call.call_type = 9 or call.call_type = 6 then 1 else 0 end
        from one_make_dwd.ciss_service_callaccept call
        left join one_make_dwb.tmp_dict call_dict on call.call_type = call_dict.dictid  and call_dict.dicttypename = '来电类型'
        left join one_make_dwb.tmp_dict process_dict on call.process_way = process_dict.dictid and process_dict.dicttypename = '来电受理单--处理方式'
        left join one_make_dwd.ciss_service_workorder workorder on workorder.dt = '20210101' and workorder.callaccept_id = call.id
        where call.dt = '20210101' and call.code != 'null';
        ```

    -   fact_call_service_task.py

        ```python
        # -*- coding:utf-8 -*-
        #__author__ = 'laowei'
        from datetime import timedelta
        
        from airflow import DAG
        from airflow.providers.apache.spark.operators.spark_sql import SparkSqlOperator
        from airflow.utils.dates import days_ago
        
        default_args = {
            'owner': 'airflow',
            'email': ['airflow@example.com'],
            'email_on_failure': False,
            'email_on_retry': False,
            'retries': 1,
            'retry_delay': timedelta(minutes=3)
        }
        
        dag = DAG(
            'fact_call_service_dag',
            default_args=default_args,
            schedule_interval='30 1 * * *',
            start_date=days_ago(1),
            dagrun_timeout=timedelta(minutes=5),
            template_searchpath=['/root/airflow/dags/OneMake/dwb'],
            tags=['itcast_sparksql']
        )
        
        fact_call_service_job = SparkSqlOperator(
            sql="fact_call_service.hql",
            master="local",
            conn_id="sparksql-airflow-connection",
            task_id="fact_call_service_job",
            dag=dag
        )
        
        fact_call_service_job
        ```

        

        